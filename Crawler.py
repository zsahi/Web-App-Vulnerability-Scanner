import requests
from bs4 import BeautifulSoup, SoupStrainer
from CoreClasses import Url
from CoreClasses import EForm
import Config
from url_normalize import url_normalize
import CommonFunctions
from urllib.parse import urlparse
from urllib.parse import urlunparse
from urllib.parse import urljoin

class Crawler:
    uEndPoints = []
    fEndpoints = []
    loginFormEndpoints = []
    urls = []

    def __init__(self):
        pass

    def init(self, url):
        self.urls = []
        self.uEndPoints = []
        self.fEndpoints = []
        self.loginFormEndpoints = []

        url = CommonFunctions.prepend_scheme(url)

        headers = {'User-Agent': Config.user_agent}
        try:
            r = requests.get(url, allow_redirects=True, headers=headers,
                             proxies={"http": Config.http_proxy, "https": Config.https_proxy}, verify=False, timeout=Config.timeout)
        except requests.exceptions.RequestException as e:
            return False

        if r.status_code != 200:
            return False

        url = r.url

        self.urls.append(Url(url, "fresh"))
        self.initial_url = url
        self.parsed_uri = urlparse(url)
        self.origin = '{uri.scheme}://{uri.netloc}'.format(uri=self.parsed_uri)
        self.domain = self.parsed_uri.netloc
        self.scheme = self.parsed_uri.scheme

        return True

    def append_url(self, url):
        flag = False
        for u in self.urls:
            if u.url == url:
                flag = True
                break
        if flag == False:
            self.urls.append(Url(url, "fresh"))
            self.uEndPoints.append(Url(url, "fresh"))

    def append_urls(self, urls):
        if urls is None:
            return
        for url in urls:
            flag = False
            for u in self.urls:
                if u.url == url:
                    flag = True
                    break
            if flag == False:
                self.urls.append(Url(url, "fresh"))
                self.uEndPoints.append(Url(url, "fresh"))

    def get_url_with_status(self, status):
        for u in self.urls:
            if u.status == status:
                return u.url
        return None

    def set_url_status(self, url, status):
        for u in self.urls:
            if u.url == url:
                u.status = status

    def findLoginPanel(self):
        index = 0
        for page in Config.loginPages:
            url = urljoin(self.origin , page)
            index = index + 1

            headers = {'User-Agent': Config.user_agent}
            try:
                r = requests.get(url, headers=headers, allow_redirects=True,
                             proxies={"http": Config.http_proxy, "https": Config.https_proxy}, verify=False, timeout=Config.timeout)
            except requests.exceptions.RequestException as e:
                continue

            if r.status_code != 200:
                continue
            old_url = url
            new_url = r.url

            soup = BeautifulSoup(r.content.decode('utf-8', errors='ignore'), "html.parser")

            for form in soup.findAll("form"):
                if not form.has_attr("action"):
                    continue
                form_action = form.get("action")

                if (form_action is None) or form_action.startswith("#"):
                    form_action = new_url

                elif form_action.startswith('//'):
                    form_action = form_action.replace('//', self.parsed_uri.scheme)

                form_action = urljoin(new_url, form_action)

                method = form.get("method")
                if method == None:
                    method = "Get"
                enctype = form.get("enctype")
                if enctype == None:
                    enctype = "application/x-www-form-urlencoded"

                fields, isLoginForm, refetch_form = CommonFunctions.extract_form_fields(form)
                if isLoginForm and len(fields["dynamic"]) <= 3:
                    self.append_login_form(EForm(form_action, method, enctype, fields,old_url, refetch_form))

                self.append_form(EForm(form_action, method, enctype, fields, old_url, refetch_form))

    def crawl(self):
        a = 1
        while True:
            if a >= Config.crawl_pages:
                break
            a = a + 1
            url = self.get_url_with_status("fresh")
            if url is None:
                break
            links = self.crawl_url(url)
            self.append_urls(links)
            self.set_url_status(url, "crawled")


    def append_form(self, eForm):
        if eForm is None or len(eForm.params) == 0:
            return
        if len(self.fEndpoints) == 0:
            self.fEndpoints.append(eForm)
            return

        for f in self.fEndpoints:
            result = CommonFunctions.forms_equal(f, eForm)
            if result == True:
                return
        self.fEndpoints.append(eForm)

    def append_login_form(self, eForm):
        if eForm is None or len(eForm.params) == 0:
            return
        if len(self.loginFormEndpoints) == 0:
            self.loginFormEndpoints.append(eForm)
            return

        for f in self.loginFormEndpoints:
            result = CommonFunctions.forms_equal(f, eForm)
            if result == True:
                return
        self.loginFormEndpoints.append(eForm)

    def crawl_url(self, url):
        results = set()
        #print("Crawling Url: ",url)
        headers = {'User-Agent': Config.user_agent}
        try:
            r = requests.get(url, headers=headers, allow_redirects=True,
                         proxies={"http": Config.http_proxy, "https": Config.https_proxy}, verify=False, timeout=Config.timeout)
        except :
            return

        if not r.url.startswith(self.origin):
            return
        new_url = r.url
        old_url = url
        try:
            soup = BeautifulSoup(r.content.decode('utf-8', errors='ignore'), "html.parser")
        except:
            return
        for link in soup.findAll('a'):
            if not link.has_attr('href'):
                continue
            url = link.get('href')
            if url == '' or url.startswith('mailto:') or url.startswith('#') or url.lower().startswith('javascript:'):
                continue
            if url.startswith('//'):
                url = url.replace('//', self.scheme)
            if '#' in url:
                url = url.split('#')[0]
            try:
                url = url_normalize(urljoin(new_url, url))
            except:
                continue

            if url.startswith(self.origin):
                results.add(url)

        for form in soup.findAll("form"):
            if not form.has_attr("action"):
                continue
            form_action = form.get("action")

            if (form_action is None) or form_action.startswith("#"):
                form_action = new_url

            elif form_action.startswith('//'):
                form_action = form_action.replace('//', self.scheme)
            try:
                form_action = url_normalize(urljoin(new_url, form_action))
            except:
                continue

            method = form.get("method")
            if method == None:
                method = "Get"
            enctype = form.get("enctype")
            if enctype == None:
                enctype = "application/x-www-form-urlencoded"

            fields, isLoginForm, refetch_form = CommonFunctions.extract_form_fields(form)
            if isLoginForm and len(fields["dynamic"]) <= 3:

                self.append_login_form(EForm(form_action, method, enctype, fields, old_url, refetch_form))

            self.append_form(EForm(form_action, method, enctype, fields, old_url, refetch_form))

        return results

